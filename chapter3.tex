% !TeX root = main.tex

\chapter{Gradual Approach: Step by Step Framework}
\label{chap:gradual_pathway}

\section{Human-inspired Video Understanding}
\label{sec:human_inspired}

\begin{figure*}[t]
  \centering
  \includegraphics[width=1\textwidth]{figures/sbs_intro_overview.pdf}
  \caption{
    Comparison between the existing methods and ours for dense video captioning.
    Similar to human interpretation, the proposed algorithm first scans a given video and explicitly determines the number of events.
    It then generates events by specifying the start and end timestamps of the salient region based on the number of events.
    Finally, it describes a specific sentence for each event region.
    On the other hand, existing methods generate a large number of event proposals and then remove duplicates with a han-crafted algorithm such as NMS.\@
    This makes it difficult to detect different events (i.e., actions) in the same time period.
  }
  \label{fig:intro_overview}
\end{figure*}

Human video understanding exhibits remarkable capabilities that current computational models struggle to replicate~\cite{Heilbron2015-ha,Krishna2017-pw}. When humans observe complex video sequences, they naturally employ a systematic cognitive process: first assessing the overall scene to estimate the number of concurrent activities, then focusing attention on specific temporal boundaries where events begin and end, and finally constructing coherent narratives that capture both individual events and their contextual relationships~\cite{wang2020event,long2021temporal}. This human-centric approach contrasts sharply with existing computational methods that rely heavily on exhaustive proposal generation followed by heuristic filtering mechanisms.

Current dense video captioning systems predominantly adopt bottom-up paradigms that generate numerous temporal proposals and subsequently eliminate redundancies using hand-crafted algorithms such as non-maximum suppression (NMS)~\cite{Krishna2017-pw,Li2018-ll,Wang2018-ap,Zhou2018-zu}. While these approaches have demonstrated effectiveness in controlled scenarios, they fundamentally diverge from human perceptual strategies and suffer from critical limitations when processing overlapping events. The reliance on temporal overlap thresholds rather than semantic content analysis leads to the inadvertent removal of distinct activities that occur within similar timeframes, thereby compromising the richness and accuracy of video descriptions~\cite{Mun2019-ap,fujita2020soda}.

The cognitive science literature suggests that human event perception involves hierarchical processing mechanisms that operate from global scene understanding to fine-grained temporal segmentation~\cite{zacks2007event,kurby2008segmentation}. Humans demonstrate superior performance in identifying event boundaries by leveraging contextual cues, semantic knowledge, and temporal dynamics simultaneously. This multi-faceted approach enables robust detection of overlapping activities and generation of coherent narratives that maintain both local accuracy and global consistency~\cite{radvansky2006event,speer2007reading}.

Motivated by these observations, we propose a human-inspired framework for dense video captioning that mirrors the natural cognitive process of video understanding. As illustrated in Figure~\ref{fig:intro_overview}, our approach implements a systematic progression from abstract scene assessment to detailed event description. Rather than generating numerous proposals indiscriminately, our method first estimates the explicit number of events present in each temporal region, providing crucial guidance for subsequent localization and captioning procedures.

Our framework operationalizes human-like video understanding through five sequential stages that reflect natural perceptual processes. Initially, the system performs global scene analysis to estimate the \textit{number of concurrent events}, departing from binary actionness classification toward explicit event counting that better captures the complexity of overlapping activities~\cite{yuan2017temporal,lin2018bsn}. Subsequently, the framework identifies \textit{event boundaries} by detecting semantic transitions and temporal discontinuities that correspond to activity onset and termination points~\cite{long2019gaussian,zhao2020bottom}. This boundary detection process enables precise temporal localization without relying on predetermined proposal sets.

The third stage synthesizes event count estimates and boundary information to generate a refined set of event proposals using an actionness-weighted temporal Intersection over Union (tIoU) algorithm, eliminating the need for conventional proposal selection mechanisms that ignore semantic content~\cite{lin2019bmn,hosang2017learning}. The fourth stage implements \textit{contextual encoding} of event proposals to ensure coherence and narrative consistency, reflecting the human tendency to construct interconnected rather than isolated event descriptions~\cite{zhu2020understanding,wang2018temporal}. Finally, the framework generates natural language descriptions through a hierarchical sentence construction process that maintains both local accuracy and global narrative coherence.

This human-inspired approach addresses fundamental limitations in existing dense video captioning systems by incorporating cognitive principles that enable robust handling of overlapping events, contextual understanding, and coherent narrative generation. The explicit modeling of human perceptual strategies provides a principled foundation for developing more effective and interpretable video understanding systems that can handle the complexity and ambiguity inherent in real-world video content~\cite{bhooshan2022multimodal,li2022adaptive}.



\section{Step-by-Step (SBS) Framework}

%In this section, we describe the core idea of SBS, dense video captioning framework.
\subsection{Overview} 
\label{subsec:method_overview}

\begin{figure*}[t]
  \centering
  \includegraphics[width=1\textwidth]{figures/sbs_design_overview.pdf}
  \caption{
    Overview of the Step-by-Step (SBS) framework for dense video captioning.
    (1) \textbf{Video encoder}: Extracts spatiotemporal features using C3D networks and encodes long-range temporal dependencies via transformer encoder.
    (2) \textbf{Temporal event counter}: Explicitly estimates the number of events at each temporal location.
    (3) \textbf{Temporal boundary classifier}: Localizes event start and end boundaries along the timeline.
    (4) \textbf{Event proposal generation}: Generates event proposals from predicted counts and boundaries without relying on hand-crafted selection algorithms.
    (5) \textbf{Context-level event encoder}: Encodes sequences of event proposals to capture inter-event dependencies.
    (6) \textbf{Sequential event captioner}: Generates natural language descriptions for each localized event using context-aware features.
  }
  \label{fig:method_overview}
\end{figure*}

The Step-by-Step (SBS) framework addresses the fundamental challenge of overlapping event detection in dense video captioning through a human-inspired progressive approach, as illustrated in Figure~\ref{fig:method_overview}. 
Unlike conventional methods that rely on proposal-based architectures with non-maximum suppression (NMS)~\cite{Krishna2017-pw,Li2018-ll}, SBS explicitly models the number of events and their temporal boundaries to achieve more precise localization of overlapping events.

The framework comprises six interconnected modules that process videos in a systematic manner.
Given an input video $V$, the \textbf{video encoder} first extracts spatiotemporal features using C3D networks~\cite{Tran2015-uq} and applies a transformer encoder~\cite{Vaswani2017-sc} to capture long-range temporal dependencies across the entire video sequence.
This encoding strategy effectively handles the temporal complexity inherent in untrimmed videos while preserving fine-grained visual information necessary for accurate event detection.

The core innovation of SBS lies in its explicit event quantification approach.
The \textbf{temporal event counter} estimates the number of overlapping events at each temporal location, providing crucial guidance for subsequent localization stages.
This differs fundamentally from binary actionness scoring used in existing methods~\cite{lin2018bsn,buch2017sst}, enabling the detection of multiple concurrent events within the same temporal region.
Subsequently, the \textbf{temporal boundary classifier} performs fine-grained localization by identifying event start and end boundaries along the timeline, leveraging the count information to guide boundary detection accuracy.

Event proposals are then generated through the \textbf{event proposal generation} module, which combines the predicted event counts and boundary classifications to produce temporally coherent event segments.
Crucially, this approach eliminates the need for hand-crafted selection algorithms such as NMS, which often suppress valid overlapping events and compromise recall performance~\cite{hosang2017learning}.

To ensure coherent and contextually appropriate caption generation, SBS employs a two-stage captioning architecture.
The \textbf{context-level event encoder}, implemented as a single-layer LSTM, processes the sequence of event proposals to capture inter-event dependencies and contextual relationships within the video narrative.
This context-aware encoding addresses the limitation that events in dense videos are not independent but form coherent temporal sequences that require understanding of their relationships.

Finally, the \textbf{sequential event captioner} generates natural language descriptions for each localized event by leveraging both the visual features of individual events and the context-encoded representations.
This sequential generation process ensures that the produced captions maintain narrative coherence and contextual appropriateness across the entire video sequence, addressing the challenge of generating fluent and coherent descriptions for multiple related events~\cite{Yu2016-il,Lei2020-tg}.

The progressive nature of the SBS framework, from explicit event counting to boundary detection and context-aware captioning, mirrors human cognitive processes in understanding complex video content.
This design choice not only improves localization accuracy for overlapping events but also enhances the interpretability of the model's decision-making process, making it more suitable for applications requiring transparent and reliable video understanding capabilities.

\subsection{Video Encoder}
\label{subsec:method_video_encoder}

The video encoder serves as the foundation of our SBS framework, responsible for extracting rich spatiotemporal representations from untrimmed videos that enable subsequent event counting and boundary classification. Given the varying lengths and complex temporal structures of untrimmed videos in dense captioning scenarios, we design a two-stage encoding architecture that combines spatial feature extraction with temporal dependency modeling.

\textbf{Video Preprocessing and Segmentation.}
For a given input video $V$, we first divide it into non-overlapping, fixed-length snippets $V=\{v_n\}_{n=1}^{T}$, where each snippet $v_n$ contains 8 consecutive frames. This snippet-based representation follows established practices in video understanding \cite{Wang2019-xv,Feichtenhofer2019-mh} and provides a manageable computational unit while preserving temporal coherence. The fixed snippet length of 8 frames balances computational efficiency with temporal granularity, as empirically validated in previous video captioning works \cite{Krishna2017-pw,Zhou2018-zu}.

The choice of non-overlapping segmentation, while potentially losing some temporal continuity, offers several advantages: (i) reduced computational complexity for long videos, (ii) clear temporal boundaries for subsequent event localization, and (iii) compatibility with standard video feature extraction pipelines \cite{Carreira2017-fg,Tran2015-uq}.

\textbf{Spatial Feature Extraction.}
Our video encoder employs a two-component architecture consisting of a CNN backbone for spatial feature extraction followed by a transformer encoder for temporal dependency modeling. This hybrid design leverages the complementary strengths of convolutional and attention-based architectures \cite{Dosovitskiy2021-vn,Arnab2021-gv}.

For spatial feature extraction, we adopt the C3D network \cite{Tran2015-uq} as our CNN backbone due to its proven effectiveness in capturing spatiotemporal patterns in video data. C3D extends traditional 2D convolutions to 3D, enabling simultaneous processing of spatial and short-term temporal information within video clips \cite{Karpathy2014-xm}. The network takes each snippet $v_n$ as input and produces feature representations $\mathbf{f}_n \in \mathbb{R}^{d_f}$, where $d_f$ denotes the feature dimension.

The complete spatial feature extraction process yields:
\begin{equation}
  \mathbf{F}_v = \text{C3D}(V) = [\mathbf{f}_1, \mathbf{f}_2, \ldots, \mathbf{f}_T] \in \mathbb{R}^{T \times d_f}
  \label{eq:spatial_features}
\end{equation}
where $T$ represents the total number of snippets in the video.

\textbf{Temporal Dependency Modeling.}
To capture long-range temporal dependencies crucial for event understanding in untrimmed videos, we employ a transformer encoder \cite{Vaswani2017-sc} with multi-head attention mechanisms. This choice is motivated by transformers' superior ability to model long-term dependencies compared to recurrent architectures \cite{Vaswani2017-sc}, which is particularly important given that videos in our datasets can extend up to 2,827 $\times$ 8 seconds (detailed in Section \ref{subsec:exp_dataset}).

Before feeding the spatial features to the transformer encoder, we apply dimensionality alignment to match the feature dimension $d_f$ with the transformer's model dimension $d_m$:
\begin{equation}
  \mathbf{H}_v^0 = \mathcal{I}_{\text{nearest}} \left( \mathbf{F}_v \right) \in \mathbb{R}^{T \times d_m}
  \label{eq:dimension_alignment}
\end{equation}
where $\mathcal{I}_{\text{nearest}}(\cdot)$ denotes nearest-neighbor interpolation for dimension transformation.

The transformer encoder then processes these aligned features through $L$ encoder layers, where each layer $l$ applies multi-head self-attention followed by a feed-forward network:
\begin{equation}
  \mathbf{H}_v^{l+1} = \text{FFN}\left(\text{LayerNorm} \left(\mathbf{H}_v^l + \text{MultiHead}\left( \mathbf{H}_v^l, \mathbf{H}_v^l, \mathbf{H}_v^l \right) \right)\right)
  \label{eq:transformer_layer}
\end{equation}

The multi-head attention mechanism computes:
\begin{equation}
  \text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) \mathbf{W}^O
  \label{eq:multihead_attention}
\end{equation}
where each attention head is defined as:
\begin{equation}
  \text{head}_i = \text{Attention}(\mathbf{Q}\mathbf{W}_i^Q, \mathbf{K}\mathbf{W}_i^K, \mathbf{V}\mathbf{W}_i^V)
  \label{eq:attention_head}
\end{equation}

The feed-forward network applies a two-layer MLP with ReLU activation \cite{He2016-qc}:
\begin{equation}
  \text{FFN}(\mathbf{x}) = \text{ReLU} \left(\mathbf{x}\mathbf{W}_1 + \mathbf{b}_1 \right)\mathbf{W}_2 + \mathbf{b}_2
  \label{eq:feed_forward_network}
\end{equation}

\textbf{Output Representation.}
The final output of the video encoder is the hidden states from the last transformer layer:
\begin{equation}
  \mathbf{H}_v = \mathbf{H}_v^L \in \mathbb{R}^{T \times d_m}
  \label{eq:final_video_representation}
\end{equation}

These contextual video representations $\mathbf{H}_v$ encode both local spatial features and global temporal dependencies, providing a comprehensive foundation for subsequent event counting and boundary classification modules in our SBS framework. The self-attention mechanism enables each snippet representation to attend to all other snippets, facilitating the detection of complex temporal patterns and event relationships necessary for accurate dense video captioning \cite{Zhou2018-zu,Wang2021-zi}.


\subsection{Temporal Event Counter}
\label{subsec:method_temporal_event_counter}

The fundamental challenge in dense video captioning lies in accurately identifying the number and temporal boundaries of overlapping events within untrimmed videos. Traditional approaches rely on proposal generation networks followed by non-maximum suppression (NMS) algorithms \cite{Krishna2017-pw,Li2018-ll}, which suffer from inherent limitations in handling multiple concurrent events due to their dependence on fixed confidence thresholds and hand-crafted post-processing heuristics \cite{hosang2017learning,lin2018bsn}.

To address these limitations, we propose the Temporal Event Counter (TEC), a novel module that explicitly estimates the number of events occurring at each temporal location in the video. Unlike binary actionness prediction methods \cite{lin2018bsn,buch2017sst} that classify temporal segments as either containing events or not, our approach provides fine-grained event count estimation, enabling the detection of multiple overlapping events within the same temporal region.

\textbf{Multi-scale Temporal Modeling.}
The TEC employs a multi-scale temporal convolutional architecture to capture event patterns across different temporal receptive fields. Given the input video features $\mathbf{H}_v \in \mathbb{R}^{L \times d}$, where $L$ represents the temporal length and $d$ denotes the feature dimension, we apply three parallel temporal convolutional branches designed to capture short-term, medium-term, and long-term temporal dependencies.

Each temporal convolutional layer is denoted as $\text{Conv}(c_n, c_k, c_s)$, where $c_n$, $c_k$, and $c_s$ represent the number of filters, kernel size, and stride, respectively. The three branches are specifically configured as:
\begin{itemize}
    \item \textbf{Short-term branch}: $\text{Conv}(512, 2, 1)$ with a temporal receptive field of approximately 0.53 seconds
    \item \textbf{Medium-term branch}: $\text{Conv}(512, 10, 5)$ with a temporal receptive field of approximately 2.67 seconds  
    \item \textbf{Long-term branch}: $\text{Conv}(512, 20, 10)$ with a temporal receptive field of approximately 5.33 seconds
\end{itemize}

These receptive field sizes are empirically determined based on the typical duration distribution of events in dense video captioning datasets \cite{Krishna2017-pw,Zhou2018-eq}, ensuring comprehensive coverage of event temporal scales.

\textbf{Event Count Prediction.}
The multi-scale feature extraction process can be formally expressed as:
\begin{align}
  \mathbf{F}_{\text{short}} &= \text{Conv}_{(512, 2, 1)} (\mathbf{H}_v) \\
  \mathbf{F}_{\text{medium}} &= \text{Conv}_{(512, 10, 5)} (\mathbf{H}_v) \\
  \mathbf{F}_{\text{long}} &= \text{Conv}_{(512, 20, 10)} (\mathbf{H}_v) \\
  \mathbf{F}_{\text{fused}} &= \text{Stack}(\mathbf{F}_{\text{short}}, \mathbf{F}_{\text{medium}}, \mathbf{F}_{\text{long}}) \\
  P_c &= \text{AvgPool}(\mathbf{F}_{\text{fused}}, N)
\end{align}

where $\text{Stack}(\cdot)$ concatenates the multi-scale features along the channel dimension with temporal alignment, and $\text{AvgPool}(\cdot, N)$ performs global average pooling to generate probability distributions over $N$ possible event counts.

The resulting event counter table $P_c = \{p_{l,n}^c\}_{l=1,n=0}^{L, N-1}$ encodes the probability of having $n$ events at temporal location $l$, where $N=15$ represents the maximum number of overlapping events plus one background class. This design choice is motivated by empirical analysis of event overlap patterns in ActivityNet Captions \cite{Krishna2017-pw}, where the maximum observed overlap rarely exceeds 14 concurrent events.

\textbf{Advantages over Traditional Approaches.}
The proposed TEC offers several key advantages over conventional event proposal methods:

\textit{Elimination of NMS dependency}: By explicitly predicting event counts rather than binary actionness scores, TEC circumvents the precision-recall trade-off inherent in NMS-based post-processing \cite{hosang2017learning}. This is particularly beneficial for detecting overlapping events, where traditional NMS algorithms tend to suppress valid proposals with high spatial-temporal overlap.

\textit{Context-aware event counting}: The multi-scale temporal modeling enables the capture of both local event characteristics and global video context, leading to more accurate event count estimation compared to single-scale approaches \cite{yuan2017temporal,long2019gaussian}.

\textit{Interpretable event localization}: The explicit event count prediction provides interpretable intermediate representations that facilitate subsequent boundary detection and caption generation, aligning with the human-inspired step-by-step reasoning process that motivates our overall framework design.

The TEC serves as the foundation for subsequent event boundary classification and caption generation modules, providing crucial guidance for determining precise temporal boundaries of the identified events.



\subsection{Temporal Boundary Classifier}
\label{subsec:method_temporal_boundary_classifier}
While the temporal event counter successfully identifies snippets containing salient actions and estimates their total number, this information alone is insufficient for precise event localization. Specifically, knowing that $N$ events exist within a video segment creates multiple possible configurations for event boundary placement, leading to ambiguous temporal segmentation. To resolve this ambiguity and achieve precise event localization, we introduce the temporal boundary classifier module.

The temporal boundary classifier aims to detect the precise temporal boundaries (i.e., start points, end points, or both) of events within each identified snippet. Unlike conventional boundary detection methods that rely on hand-crafted features~\cite{lin2018bsn} or non-maximum suppression~\cite{hosang2017learning}, our approach employs a learning-based two-stream architecture that captures both actionness and boundary-specific features.

As illustrated in Figure~\ref{fig:method_overview}, the temporal boundary classifier consists of two parallel streams with complementary objectives. The first stream reuses the convolutional layers from the temporal event counter with fixed parameters, preserving the learned actionness representations that effectively capture salient temporal regions. This design choice ensures consistency with the event counting stage while avoiding redundant parameter learning. The second stream employs multiple dense temporal 1D convolutional layers with small kernel sizes and strides, specifically designed to capture fine-grained temporal transitions that characterize event boundaries~\cite{lin2019bmn,zhao2020bottom}.

The architectural design reflects our intuition that effective boundary detection requires both global actionness awareness and local temporal precision. The actionness stream provides semantic understanding of event-relevant regions, while the dense temporal stream focuses on detecting rapid changes in visual content that typically occur at event transitions~\cite{long2019gaussian,yuan2017temporal}. 

For each snippet $s_l$ in the input sequence, the temporal boundary classifier outputs start and end probabilities:
$$P_b = \{(p_{l,s}^b, p_{l,e}^b)\}_{l=1}^{L}$$
where $p_{l,s}^b$ and $p_{l,e}^b$ represent the probabilities that snippet $l$ contains an event start and end boundary, respectively. We refer to $P_b$ as the event boundary table, which serves as input to the subsequent event localization and captioning stages.

The training process follows the same optimization framework as the temporal event counter, but targets boundary-specific ground truth labels derived from temporal annotations. This modular design enables independent optimization of counting and boundary detection objectives while maintaining end-to-end differentiability when integrated with the complete SBS framework.

\subsection{Event Proposal Generation}%
\label{subsec:method_event_proposal_generation}

Building upon the outputs from the Temporal Event Counter (TEC) and Temporal Boundary Classifier (TBC) modules described in Sections~\ref{subsec:method_temporal_event_counter} and~\ref{subsec:method_temporal_boundary_classifier}, we now present our event proposal generation algorithm that leverages both event count predictions and boundary classifications to produce high-quality event proposals without relying on traditional post-processing techniques.

\textbf{Actionness Score Computation.}
Given the event counter table $P_c$ and event boundary table $P_b$ from the previous modules:
\begin{align}
  P_c &= \{p_{l,n}^c\}_{l=1,n=1}^{L, N-1} \\
  P_b &= \{(p_{l,s}^b, p_{l,e}^b)\}_{l=1}^{L}
\end{align}
we first construct a one-dimensional actionness table $T_a$ that quantifies the expected number of events occurring at each temporal location. The actionness score for each snippet $l$ is computed as the weighted sum of event counts, where each count $n$ is weighted by its corresponding probability:
\begin{align}
  T_{a}[l] = \sum_{n=0}^{N-1} n \cdot p_{l,n}^c
\end{align}
This formulation provides a continuous measure of event density that reflects both the likelihood and magnitude of event occurrences at each temporal position.

\textbf{Candidate Proposal Generation.}
Unlike traditional methods that generate dense proposal sets followed by non-maximum suppression (NMS) \cite{hosang2017learning}, our approach constructs candidate proposals directly from high-confidence boundary predictions. We define the set of candidate event proposals as:
\begin{align}
  \hat{E} = \{(\iota_s, \iota_e) \mid \iota_s < \iota_e, \, p_{\iota_s,s}^b \geq \tau_b, \, p_{\iota_e,e}^b \geq \tau_b\}
\end{align}
where $\iota_s$ and $\iota_e$ represent the start and end indices of candidate proposals, respectively, and $\tau_b = 0.5$ is the confidence threshold for boundary classification. This construction ensures that only high-confidence temporal boundaries are considered for proposal formation.

\textbf{Content-Aware Proposal Selection.}
To select meaningful event proposals from the candidate set $\hat{E}$, we employ a content-aware filtering mechanism based on the computed actionness scores. For each candidate proposal $(\iota_s, \iota_e) \in \hat{E}$, we calculate the average actionness score across its temporal span:
\begin{align}
  S(\iota_s, \iota_e) = \frac{1}{\iota_e - \iota_s + 1} \sum_{i=\iota_s}^{\iota_e} T_a[i]
\end{align}
The final set of event proposals consists of candidates whose average actionness score exceeds unity:
\begin{align}
  E = \{(\iota_s, \iota_e) \in \hat{E} \mid S(\iota_s, \iota_e) > 1\}
\end{align}
This threshold ensures that selected proposals contain, on average, at least one significant event occurrence.

\textbf{Advantages Over Traditional Approaches.}
Our event proposal generation algorithm offers several key advantages over conventional methods employed in dense video captioning \cite{Krishna2017-pw, Li2018-ll, Wang2018-ap}:

\textit{Content-Aware Selection:} Unlike NMS-based approaches \cite{hosang2017learning} that rely solely on temporal intersection over union (tIoU) metrics for duplicate removal, our method considers the actual video content through actionness scores.
This content-aware selection mechanism prevents the elimination of semantically important but temporally overlapping events, which is particularly crucial for complex video scenarios where multiple concurrent activities occur \cite{Mun2019-ap}.

\textit{Computational Efficiency:} Our approach generates proposals in a single forward pass without requiring exhaustive proposal enumeration followed by post-processing. This eliminates the computational overhead associated with traditional two-stage methods \cite{buch2017sst, lin2018bsn} while maintaining proposal quality.

\textit{Interpretability:} The explicit modeling of event counts and boundary classifications provides interpretable intermediate representations that facilitate debugging and analysis. This contrasts with end-to-end approaches where proposal generation mechanisms remain opaque \cite{Zhou2018-zu}.

\textit{Overlapping Event Handling:} By avoiding hard suppression mechanisms like NMS, our method naturally accommodates overlapping events that frequently occur in real-world scenarios. This capability is essential for comprehensive video understanding where simultaneous activities are common \cite{Duan2018-qf}.

The proposed event proposal generation framework thus provides a principled and efficient approach to temporal event localization that addresses key limitations of existing methods while maintaining computational tractability.

\subsection{Sentence Generation}
Dense video captioning requires generating contextually coherent descriptions that not only accurately describe individual events but also maintain logical consistency across multiple events within the same video~\cite{Krishna2017-pw}. Unlike traditional video captioning approaches that generate isolated descriptions~\cite{venugopalan2015sequence,venugopalan2015translating}, dense video captioning necessitates understanding inter-event relationships and temporal dependencies to produce contextually aware captions~\cite{yu2016video,lei2020mart}.

\textbf{Motivation for Context-Aware Generation.}
Real-world videos contain events that are temporally related and semantically interdependent. A key challenge in dense video captioning is ensuring that generated descriptions maintain coherence across events while accurately reflecting temporal transitions and causal relationships~\cite{xiong2018move}. For instance, when describing sequential actions such as "a person starts cooking" followed by "he adds ingredients," the model must understand the contextual relationship between these events to generate appropriate transitional phrases and maintain narrative consistency. Previous approaches that generate captions independently for each event often fail to capture such contextual nuances, resulting in repetitive or incoherent descriptions~\cite{mun2019streamlined}.

\textbf{Two-Level Hierarchical Architecture.}
To address these challenges, our sentence generation module adopts a two-level hierarchical approach that explicitly models both local event contexts and global video-level dependencies~\cite{pan2016hierarchical,baraldi2017hierarchical}. This design enables the model to generate contextually rich descriptions by: (1) encoding comprehensive contextual information that captures both intra-event and inter-event relationships, and (2) leveraging this encoded context during sequential caption generation to maintain coherence across multiple event descriptions.

As illustrated in Figure~\ref{fig:method_overview}, our framework employs two specialized LSTM networks: a context-level event encoder ($\text{LSTM}_c$) and a sequential event captioner ($\text{LSTM}_s$). The context-level event encoder processes video features corresponding to detected event proposals and generates enriched contextual representations that capture temporal dependencies and semantic relationships between events. The sequential event captioner then utilizes these contextual representations to generate coherent descriptions while maintaining awareness of previously generated captions.

\textbf{Context Encoding Process.}
The context-level event encoder takes video features $\mathbf{F}_v \in \mathbb{R}^{T \times d_f}$ corresponding to event proposals and encodes comprehensive contextual information for a sequence of events, where $T$ represents the temporal duration of the event proposal and $d_f$ denotes the feature dimension. This encoder aggregates multi-scale temporal information and establishes contextual relationships that are essential for generating coherent event descriptions~\cite{Chen2020-di,Wang2021-xe}.

\textbf{Sequential Caption Generation.}
The sequential event captioner leverages the encoded contextual information to generate word-by-word descriptions while maintaining awareness of both the current event context and previously generated captions. This sequential processing ensures that generated descriptions exhibit temporal coherence and appropriate use of contextual cues such as transitional phrases and referential expressions~\cite{Huang2020-as}.

Given the $t$-th event proposal $e_t$, the sentence generation process is formulated as follows:
\begin{align}
  r_t &= \text{LSTM}_c(\mathbf{F}_v(e_t), g_{t-1}, r_{t-1}) \\
  g_t &= \text{LSTM}_s(\mathbf{F}_v(e_t), r_t, g_{t-1}),
\end{align}
where $r_t$ represents the encoded contextual feature for the $t$-th event proposal, $g_t$ denotes the hidden state of the sequential captioner, and $\mathbf{F}_v(\cdot)$ is a function that extracts video features corresponding to the input event proposal. The contextual encoder $\text{LSTM}_c$ integrates visual features from the current event with contextual information from previous events ($g_{t-1}$) and previous contextual states ($r_{t-1}$), enabling the model to maintain long-term dependencies across event sequences. The sequential captioner $\text{LSTM}_s$ then generates natural language descriptions by combining current visual features, encoded context, and previous hidden states, ensuring both accuracy and contextual coherence in the generated captions.

\section{Experimental Results and Discussion}
\subsection{Dataset}
\label{subsec:exp_dataset}
We evaluated the performance of SBS on the ActivityNet Captions dataset \cite{krishna2017dense}, which contains 19,994 YouTube videos.
The dataset is divided into three subsets for training, validation, and testing, consisting of 10,009, 4,917, and 4,885 videos, respectively.
The videos range in length from short to long, with minimum, average, and maximum lengths of 1.58, 117.60, and 755.11, respectively.
The number of events per video ranges from 2 to 27, with minimum, average, and maximum values of 2, 3.66, and 27.
The lengths of the sentences in the dataset range from 17 to 409 words, with an average of 67.7 words.

\subsection{Metrics}
To evaluate the performance of SBS event localization, we compare the temporal intersection over union (tIoU) between ground-truth events and predicted events.
We measure recall and precision for thresholds of 0.3, 0.5, 0.7, and 0.9. Specifically, we consider the sample to be true if the tIoU between the two is above each threshold.
For captioning performance evaluation, we use three metrics: METEOR \cite{banerjee2005meteor}, CIDEr \cite{vedantam2015cider}, and BLEU \cite{papineni2002bleu}.
We use publicly available evaluation code\footnote{\url{https://github.com/ranjaykrishna/densevid_eval}} provided by the ActivityNet Captions Challenge.
Given an event and sentence pair, we calculate a captioning score by comparing the corresponding ground-truth sentence if the tIoU between the predicted event and any ground-truth event is greater than the threshold. Otherwise, the score is set to 0.

\subsection{Implementation Details}
\label{subsec:exp_impl}
The transformer of SBS's video encoder follows the original paper \cite{vaswani2017attention}.
Specifically, we set the hidden size $d_m$ of multi-head attention to 512, the number of attention heads to 8, and the number of layers in the encoder to 6. The feed-forward network has 2,048 nodes.
The dropout rate for residual blocks and attention is set to 0.1.
Following previous works \cite{mun2019streamlined,wang2021end}, we set the hidden size of the event context encoder and sequential captioner in the captioning network to a single layer of 512.
To prevent overfitting and improve generalization, we use PRELU \cite{he2015delving} and GELU \cite{hendrycks2016gaussian} as activation functions for the CNNs and fully-connected layers for the temporal event counter and temporal boundary classifier, which experimentally outperforms other options.
We set the epochs for each stage of SBS to 20, 20, and 30, and train with an adamW optimizer \cite{loshchilov2017decoupled} and a batch size of 1.
The focal loss hyperparameter, $\gamma$, is set to 2.0 (more information on $\gamma$ settings is in Section \ref{subsec:exp_ablation}).
Several recently proposed methods \cite{mun2019streamlined,wang2020event,deng2021sketch} use reinforcement learning (RL) to further improve the captioning module.
To ensure fair comparison, we also fine-tune the context-level event encoder and sequential event captioner using RL (based on \cite{rennie2017self}) with the reward function METEOR.

\subsection{Performance Comparison}
\label{subsec:exp_comparison}

\textbf{Event Localization.}
We compared several state-of-the-art dense video captioning methods to evaluate the performance of SBS. First, we present the performance of event localization in Table \ref{tab:eval_event_localizer}.
Above all, SBS shows the best F1 score performance among other methods.
MFT and SDVC generate candidate event proposals, then use an event selection network, such as ESGN, to remove less significant or overlapping ones. On the other hand, PDVC and PPVC generate proposals directly in parallel from a localization head composed of box prediction and classification.
In contrast, SBS has a gradual approach to explicitly estimate the number of events, detect event boundaries, and then generate event proposals. This approach is more effective in localizing overlapping or short events.
Moreover, SBS can detect multiple context events in a video, enabling detailed video descriptions (details in Section \ref{subsec:exp_qualitative}).
SBS outperforms MFT by a large margin and is superior to both SDVC and PDVC in terms of overall localization performance and F1 score.

\textbf{Dense Captioning.}
Table \ref{tab:eval_captioner} shows the performance of state-of-the-art methods in dense video captioning.
When using ground-truth event proposals, SBS achieves a remarkable improvement compared to other methods in terms of the METEOR score, which is a commonly used evaluation metric in the ActivityNet Captions Challenge.
When using predicted proposals, SBS achieves comparable performance to state-of-the-art algorithms on CIDEr and METEOR.
Specifically, it exceeds SGR by 5.8 on CIDEr and is only 0.02 below SGR on METEOR.
In particular, SBS outperforms MV-GPT or Vid2Seq with pre-trained models.
These results prove that both the context-level event encoder and sequential captioner, which consist of two LSTMs, provide excellent captioning quality.

\begin{sidewaystable}
% \begin{table}[t]
  \centering
  \caption{
    Performance comparison of the event localization with respect to the 4 temporal intersection of unions (@tIoU) thresholds on the Activity Captions validation set.
  }
  \begin{tabular}{@{}l|ccccc|ccccc|c@{}}
    \hline
    \multirow{2}{*}{Method}        & \multicolumn{5}{|c|}{Recall (@tIoU)} & \multicolumn{5}{|c|}{Precision (@tIoU)} & \multirow{2}{*}{F1}                                                                     \\
                                   & @0.3                                 & @0.5                                    & @0.7                & @0.9  & Average & @0.3  & @0.5  & @0.7  & @0.9  & Average &       \\
    \hline
    MFT \cite{xiong2018move}       & 46.18                                & 29.76                                   & 15.54               & 5.77  & 24.31   & 86.34 & 68.79 & 38.30 & 12.19 & 51.41   & 33.01 \\
    SDVC \cite{mun2019streamlined} & 93.41                                & 76.40                                   & 42.40               & 10.10 & 55.58   & 96.71 & 77.73 & 44.84 & 10.99 & 57.57   & 56.56 \\
    PDVC \cite{wang2021end}        & 89.47                                & 81.91                                   & 44.63               & 15.67 & 55.42   & 97.16 & 78.09 & 42.68 & 14.40 & 58.07   & 56.71 \\
    PPVC \cite{choi2022parallel}   & 91.71                                & 78.90                                   & 56.73               & 20.60 & 61.98   & 96.23 & 73.80 & 37.66 & 12.61 & 55.07   & 58.33 \\
    Vid2Seq \cite{yang2023vid2seq} & -                                    & -                                       & -                   & -     & 52.7    & -     & -     & -     & -     & 53.9    & 53.29 \\
    \textbf{SBS}                   & 93.14                                & 82.29                                   & 45.70               & 15.88 & 59.25   & 96.85 & 79.68 & 43.56 & 11.47 & 57.89   & 58.56 \\
    \hline
  \end{tabular}
  \label{tab:eval_event_localizer}
% \end{table}
\end{sidewaystable}

% \begin{table*}[t]
\begin{sidewaystable}
  \centering
  \caption{
    A summary of the performance comparison using BLEU, CIDEr and METEOR on the ActivityNet validation set.
    We present the performances obtained from both learned and ground-truth events.
    Asterisk (*) indicates methods evaluated on incomplete dataset (e.g., 80\%) due to download issues.
    CE and RL stand for cross-entropy and reinforcement learning, respectively.
  }
  \begin{tabular}{@{}l|c|cc|ccc|ccc@{}}
    \hline
    \multirow{2}{*}{Method}             & \multirow{2}{*}{Pretrained} & \multicolumn{2}{|c|}{Training} & \multicolumn{3}{|c|}{with GT proposals} & \multicolumn{3}{c}{with predicted proposals}                                                 \\
                                        &                             & CE                             & RL                                      & BLEU                                         & CIDEr   & METEOR  & BLEU   & CIDEr   & METEOR \\
    \hline
    DCE \cite{krishna2017dense}         &                             & $\checkmark$                   &                                         & 1.60                                         & 25.12   & 8.88    & 0.71   & 12.43   & 5.69   \\
    MFT \cite{xiong2018move}            &                             & $\checkmark$                   &                                         & 1.24                                         & 21.00   & 7.08    & 1.15   & 9.25    & 4.98   \\
    TDA-CG \cite{wang2018bidirectional} &                             & $\checkmark$                   &                                         & -                                            & -       & -       & 1.31   & 7.99    & 5.86   \\
    DVC \cite{li2018jointly}            &                             & $\checkmark$                   &                                         & 1.62                                         & 25.24   & 10.33   & 0.73   & 12.61   & 6.93   \\
    Efficient \cite{suin2020efficient}  &                             & $\checkmark$                   &                                         & -                                            & -       & -       & 1.35   & 13.82   & 6.21   \\
    % MT \cite{zhou2018end}               & TSN & $\checkmark$ & & 2. &&& 1.15 & 9.25 & 4.98  \\
    SDVC \cite{mun2019streamlined}      &                             & $\checkmark$                   & $\checkmark$                            & 1.28                                         & 47.71   & 13.07   & 0.93   & 30.68   & 8.82   \\
    ECHR \cite{wang2020event}           &                             & $\checkmark$                   &                                         & 1.96                                         & 39.73   & 10.58   & 1.29   & 14.71   & 7.19   \\
    WS-DEC \cite{chen2021towards}*      &                             & $\checkmark$                   &                                         & -                                            & -       & -       & 1.33   & 21.21   & 7.49   \\
    SGR \cite{deng2021sketch}           &                             & $\checkmark$                   & $\checkmark$                            & -                                            & -       & -       & 1.67   & 22.12   & 9.07   \\
    PDVC \cite{wang2021end}             &                             & $\checkmark$                   &                                         & 2.64                                         & 47.26   & 10.54   & 1.65   & 25.87   & 7.50   \\
    {MV-GPT \cite{seo2022end}}          & \checkmark                  & {\checkmark}                   & {}                                      & {6.84}                                       & {-}     & {12.31} & {-}    & {-}     & {-}    \\
    {PPVC \cite{choi2022parallel}}      & {}                          & {{\checkmark}}                 & {}                                      & {2.76}                                       & {49.31} & {10.48} & {1.68} & {23.02} & {7.91} \\
    {Vid2Seq \cite{yang2023vid2seq}}    & {\checkmark}                & {{\checkmark}}                 & {}                                      & {-}                                          & {-}     & {-}     & {-}    & {30.1}  & {8.5}  \\
    \textbf{SBS}                        &                             & $\checkmark$                   & $\checkmark$                            & 1.22                                         & 41.82   & 14.27   & 1.08   & 27.92   & 9.05   \\
    \hline
  \end{tabular}
  \label{tab:eval_captioner}
\end{sidewaystable}
% \end{table*}


\textbf{Inference Time.}
We compare the inference time of SBS with that of other methods in Table \ref{tab:eval_inference_time}.
The inference time of SBS is 1.13 seconds, faster than TDA-CG and MT, but slower than PDVC.
This can be explained by the fact that SBS does not generate unnecessary event proposals unlike previous works.
Secondly, SBS generates event proposals with only one feed-forward without repetitive inferences for event proposal generation.
However, compared to PDVC, which decodes captioning and localization in parallel as, SBS is inherently inferior.

\subsection{Qualitative Results}
\label{subsec:exp_qualitative}

We visualize the results for two videos included in the ActivityNet Captions validation set in Figure \ref{fig:experiment_qualitative} to examine the results of SBS in more detail through qualitative evaluation.
It indicates two main features that can clarify SBS.
First, SBS is highly effective at detecting events that occur at the same time.
For instance, in the first video, the red and blue events have a temporal Intersection over Union (tIoU) of over 0.8, but SBS recognizes them as distinct events and retains both. Algorithms that don't take content into account, like NMS, would remove one of the events.
In the second video, SBS creates temporally overlapped events and provides different captions, demonstrating its ability to caption multiple stories that unfold simultaneously in the video.
Second, the 1d actionness table approximately represents the number of scene transitions and duplicate events in the video story.
In the first video, it is evident that the black event interval (where three events overlap) has the highest actionness.
In the second video, we observe a pattern where actionness rapidly decreases whenever the scene changes and remains consistent when the scene is maintained.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/sbs_exp_qualitative}
  \caption{
    Examples of qualitative result on the ActivityNet Captions dataset.
    Sentences corresponding to the event are matched with the same color.
    We also show the 1D actionness table obtained from the event counter table.
    % (details are in Section \ref{subsec:method_temporal_event_counter}).
  }
  \label{fig:experiment_qualitative}
\end{figure*}


\begin{table}[t]
  \parbox{\linewidth}{
    \centering
    \caption{
      Ablation results on the ActivityNet Captions validation set.
      TEC, TBC, NMS stand for the temporal event counter, the temporal boundary classifier, and non-maximum suppression algorithm, respectively.
      R, P, M stand for recall, precision, and F1 score, respectively.
    }
    \begin{tabular}{ccc|c|c|c|c}
      \hline
      \multicolumn{3}{c|}{Applied modules} & \# of        & \multirow{2}{*}{R} & \multirow{2}{*}{P} & \multirow{2}{*}{M}                \\
      TEC                                  & TBC          & NMS                & proposals          &                    &              \\
      \hline
      $\checkmark$                         &              & $\checkmark$       & 2.57               & 44.32              & 49.03 & 6.64 \\
                                           & $\checkmark$ &                    & 2.77               & 51.81              & 53.09 & 6.93 \\
      $\checkmark$                         & $\checkmark$ &                    & 3.54               & 59.25              & 58.89 & 9.05 \\
      \hline
    \end{tabular}
    \label{tab:exp_ablation_module}
  }
\end{table}

\begin{table}[t]
  \parbox{\linewidth}{
    \centering
    \caption{
      Ablation study on focal loss weight $\gamma$.
      % in Section \ref{subsec:method_temporal_event_counter}.
      We report F1 scores for tIoU thresholds of 0.3, 0.5, 0.7, and 0.9.
    }
    \setlength\tabcolsep{6pt}
    \begin{tabular}{c|cccc|c}
      \hline
      \multirow{2}{*}{$\gamma$} & \multicolumn{4}{c|}{F1 (@tIoU)} & \multirow{2}{*}{Average}                                                 \\
                                & @0.3                            & @0.5                     & @0.7          & @0.9          &               \\
      \hline
      0.5                       & \textbf{95.9}                   & \textbf{82.9}            & 42.0          & 11.7          & 58.1          \\
      1.0                       & 95.4                            & 81.4                     & 43.6          & 11.5          & 58.0          \\
      2.0                       & 94.9                            & 80.9                     & \textbf{44.6} & \textbf{13.3} & \textbf{58.4} \\
      5.0                       & 91.4                            & 77.4                     & 42.61         & 12.2          & 55.9          \\
      \hline
    \end{tabular}
    \label{tab:exp_ablation_gamma}
  }
\end{table}

\begin{table}[t]
  \centering
  \caption{
    {Comparison of SBS's inference time with existing methods.
        We measure the average inference time per video on the ActivityNet Captions dataset with a single RTX 3090 GPU.}
  }
  \begin{tabular}{l|cccc}
    \hline
    {Method}   & {TDA-CG \cite{wang2018bidirectional}} & {MT \cite{zhou2018end}} & {PDVC \cite{wang2021end}} & {{SBS}}  \\
    \hline
    {Time (s)} & {2.28}                                & {1.97}                  & {0.18}                    & {{1.13}} \\
    \hline
  \end{tabular}
  \label{tab:eval_inference_time}
\end{table}

\subsection{Ablation Study}
\label{subsec:exp_ablation}

We conduct several ablation studies on the ActivityNet Captions validation set to verify the effectiveness of SBS's each modules.
We compare three models of modules in different combinations as follows: (i) SBS without the temporal boundary classifier, (ii) SBS without the temporal event counter, (iii) Full model.
Specifically, model (i) generates a large number of valid proposals from the event counter map and removes duplicates with a non-maximum suppression algorithm because information around the boundary is ambiguous.
Model (ii) generates event proposals from a binary actionness map without inferring an explicit number of events.
Model (iii) is a full SBS using all modules.
The results are summarized in Table \ref{tab:exp_ablation_module}.
Model (i) detects fewer events per video due to the elimination of unnecessary event proposals, which eventually leads to low recall.
Furthermore, ambiguous event boundaries hinder high-quality localization.
Since model (ii) generates event proposals using actionness maps composed of positive or negative elements (threshold is 0.5), it is difficult to detect events in overlapping or different contexts.
The full model successfully localizes events and leads to improved performance in terms of METEOR.
These results demonstrate that both the temporal event counter and temporal boundary classifier, which are modules for localization of SBS, are effective in detecting multiple overlapping events of dense video captioning.

We vary the focal loss hyperparameter, $\gamma \in \left[0.5, 1.0, 2.0, 5.0\right]$ used in \cite{lin2017focal}.
We report the F1 score of the event localization for four trained model.
The results are presented in Table \ref{tab:exp_ablation_gamma}.
With an F1 difference of up to 2.6 over a wide range of $\gamma$, SBS shows stable results.
Based on these results, we set $\gamma$ to 2.0 for the best performance.